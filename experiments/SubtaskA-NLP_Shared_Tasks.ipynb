{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moderate-incidence",
   "metadata": {
    "id": "moderate-incidence"
   },
   "source": [
    "# Experiments!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-revelation",
   "metadata": {
    "id": "expired-revelation"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "essential-footage",
   "metadata": {
    "id": "essential-footage"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-private",
   "metadata": {
    "id": "cheap-private"
   },
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "OLxIwxEMDIZu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLxIwxEMDIZu",
    "outputId": "cac10e38-27ac-41b2-c409-3cc021eb57e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'datasets/train'...\n",
      "remote: Enumerating objects: 3083, done.\u001b[K\n",
      "remote: Counting objects: 100% (3083/3083), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2728/2728), done.\u001b[K\n",
      "remote: Total 6864 (delta 567), reused 2504 (delta 279), pack-reused 3781\u001b[K\n",
      "Receiving objects: 100% (6864/6864), 157.36 MiB | 20.74 MiB/s, done.\n",
      "Resolving deltas: 100% (660/660), done.\n",
      "Cloning into 'datasets/validation'...\n",
      "remote: Enumerating objects: 1636, done.\u001b[K\n",
      "remote: Counting objects: 100% (1636/1636), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1438/1438), done.\u001b[K\n",
      "remote: Total 1636 (delta 580), reused 1224 (delta 170), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (1636/1636), 27.88 MiB | 20.85 MiB/s, done.\n",
      "Resolving deltas: 100% (580/580), done.\n",
      "Cloning into 'datasets/test'...\n",
      "remote: Enumerating objects: 2508, done.\u001b[K\n",
      "remote: Total 2508 (delta 0), reused 0 (delta 0), pack-reused 2508\u001b[K\n",
      "Receiving objects: 100% (2508/2508), 215.28 MiB | 18.43 MiB/s, done.\n",
      "Resolving deltas: 100% (54/54), done.\n",
      "Checking out files: 100% (2060/2060), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf datasets\n",
    "!mkdir datasets\n",
    "!git clone https://github.com/ncg-task/training-data.git datasets/train\n",
    "!git clone https://github.com/ncg-task/trial-data.git datasets/validation\n",
    "!git clone https://github.com/ncg-task/test-data datasets/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "scenic-hearts",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scenic-hearts",
    "outputId": "6e0573b8-671c-41b4-b793-f71160938183"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete query_wellformedness 1\n",
      "Training complete passage_re-ranking 2\n",
      "Training complete part-of-speech_tagging 8\n",
      "Training complete sentence_compression 4\n",
      "Training complete sentiment_analysis 52\n",
      "Training complete temporal_information_extraction 2\n",
      "Training complete phrase_grounding 1\n",
      "Training complete text_generation 6\n",
      "Training complete text-to-speech_synthesis 3\n",
      "Training complete smile_recognition 1\n",
      "Training complete topic_models 1\n",
      "Training complete question_generation 2\n",
      "Training complete relation_extraction 14\n",
      "Training complete paraphrase_generation 2\n",
      "Training complete question_similarity 1\n",
      "Training complete question_answering 6\n",
      "Training complete sentence_classification 3\n",
      "Training complete prosody_prediction 1\n",
      "Training complete semantic_role_labeling 5\n",
      "Training complete text_summarization 15\n",
      "Training complete semantic_parsing 3\n",
      "Training complete sarcasm_detection 2\n",
      "Training complete natural_language_inference 101\n",
      "Training complete negation_scope_resolution 1\n"
     ]
    }
   ],
   "source": [
    "input_dir = \"datasets/train/\"\n",
    "\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "list_of_folders = [\"query_wellformedness\", \"passage_re-ranking\", \"part-of-speech_tagging\", \n",
    "         \"sentence_compression\", \"sentiment_analysis\", \"temporal_information_extraction\", \n",
    "         \"phrase_grounding\", \"text_generation\", \"text-to-speech_synthesis\", \n",
    "         \"smile_recognition\", \"topic_models\", \"question_generation\", \n",
    "         \"relation_extraction\", \"paraphrase_generation\", \"question_similarity\", \n",
    "         \"question_answering\", \"sentence_classification\", \"prosody_prediction\", \n",
    "         \"semantic_role_labeling\", \"text_summarization\", \"semantic_parsing\",\n",
    "         \"sarcasm_detection\", \"natural_language_inference\", \"negation_scope_resolution\"]\n",
    "input_stanza_list = []\n",
    "input_stanza_len = []\n",
    "input_sent_num_list = []\n",
    "file_name_list = []\n",
    "for fls in list_of_folders:\n",
    "  count=0\n",
    "  for i in os.listdir(input_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    for files in os.listdir(input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None,map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        input_stanza_len.append(len(stanza_lines_list))\n",
    "        input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        input_sent_num_list.append(sentence_num_list)\n",
    "    file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"Training complete\",fls, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "friendly-alias",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "friendly-alias",
    "outputId": "e1f40f9a-532e-48c6-b016-aec1745a09d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid complete machine-translation 10\n",
      "Valid complete named-entity-recognition 10\n",
      "Valid complete question-answering 10\n",
      "Valid complete relation-classification 10\n",
      "Valid complete text-classification 10\n"
     ]
    }
   ],
   "source": [
    "val_input_dir = \"datasets/validation/\"\n",
    "val_list_of_folders = [\"machine-translation\", \"named-entity-recognition\", \"question-answering\",\n",
    "         \"relation-classification\", \"text-classification\"]\n",
    "val_input_stanza_list = []\n",
    "val_input_sent_num_list = []\n",
    "val_file_name_list = []\n",
    "val_input_stanza_len = []\n",
    "for fls in val_list_of_folders:\n",
    "  count=0\n",
    "  for i in os.listdir(val_input_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    for files in os.listdir(val_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        val_input_stanza_len.append(len(stanza_lines_list))\n",
    "        val_input_stanza_list.append(stanza_lines_list)\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(val_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        val_input_sent_num_list.append(sentence_num_list)\n",
    "    val_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"Valid complete\",fls,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "critical-upper",
   "metadata": {
    "id": "critical-upper"
   },
   "outputs": [],
   "source": [
    "input_sent_num_list = [[int(s) for s in sublist] for sublist in input_sent_num_list] # convert sentence list string to integer\n",
    "input_sent_num_list = [list(set(x)) for x in input_sent_num_list]\n",
    "\n",
    "#### conversion for Valid set\n",
    "val_input_sent_num_list = [[int(s) for s in sublist] for sublist in val_input_sent_num_list] # convert sentence list string to integer\n",
    "val_input_sent_num_list = [list(set(x)) for x in val_input_sent_num_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tropical-target",
   "metadata": {
    "id": "tropical-target"
   },
   "outputs": [],
   "source": [
    "#### Ground truth label formation for classification\n",
    "#### Train set\n",
    "multihot_input_sent = []\n",
    "for i in range(len(input_stanza_list)):\n",
    "  temp =[0]*input_stanza_len[i]\n",
    "  for j in range(len(input_sent_num_list[i])):\n",
    "    t1 = input_sent_num_list[i][j] -1\n",
    "    temp[t1] = 1\n",
    "  multihot_input_sent.append(temp)\n",
    "#### valid set\n",
    "val_multihot_input_sent = []\n",
    "for i in range(len(val_input_stanza_list)):\n",
    "  temp =[0]*val_input_stanza_len[i]\n",
    "  for j in range(len(val_input_sent_num_list[i])):\n",
    "    t1 = val_input_sent_num_list[i][j] -1\n",
    "    temp[t1] = 1\n",
    "  val_multihot_input_sent.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stone-barrier",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stone-barrier",
    "outputId": "8944f754-804a-4693-e566-4f3eea30dd17"
   },
   "outputs": [],
   "source": [
    "## Function for flattening 2d list to 1d list\n",
    "from collections import Iterable\n",
    "def flatten(lis):\n",
    "  for item in lis:\n",
    "    if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "      for x in flatten(item):\n",
    "        yield x\n",
    "    else:        \n",
    "      yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "going-feature",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "going-feature",
    "outputId": "8787c2c0-ce8c-4754-b7f9-1b72bc78a6f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51538\n",
      "train size 51296 51296\n",
      "valid size 10681 10681\n"
     ]
    }
   ],
   "source": [
    "#### Flatten the list and use heuristic to remove vague sentences\n",
    "#### Train set\n",
    "from collections import Counter\n",
    "\n",
    "train_sentences = list(flatten(input_stanza_list))\n",
    "train_label = list(flatten(multihot_input_sent))\n",
    "\n",
    "train_tuple = list(set((zip(train_sentences,train_label))))\n",
    "print(len(train_tuple))\n",
    "train_in_sentences = []\n",
    "train_sent_label = []\n",
    "for stan,lab in train_tuple:\n",
    "  if len(stan) >4:\n",
    "    train_in_sentences.append(stan)\n",
    "    train_sent_label.append(lab)\n",
    "\n",
    "valid_sentences = list(flatten(val_input_stanza_list))\n",
    "valid_label = list(flatten(val_multihot_input_sent))\n",
    "\n",
    "valid_tuple = list(set((zip(valid_sentences,valid_label))))\n",
    "valid_in_sentences = []\n",
    "valid_sent_label = []\n",
    "for stan,lab in valid_tuple:\n",
    "  if len(stan) >4:\n",
    "    valid_in_sentences.append(stan)\n",
    "    valid_sent_label.append(lab)\n",
    "\n",
    "print(\"train size\",len(train_in_sentences),len(train_sent_label))\n",
    "print(\"valid size\",len(valid_in_sentences),len(valid_sent_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "respected-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "label0_sent = [i for i, j in zip(train_in_sentences, train_sent_label) if j == 0]\n",
    "label1_sent = [i for i, j in zip(train_in_sentences, train_sent_label) if j == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "quarterly-zealand",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46271 5025\n"
     ]
    }
   ],
   "source": [
    "print(len(label0_sent), len(label1_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fgUvGEgGNMV3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgUvGEgGNMV3",
    "outputId": "a500d178-d931-47c4-835f-9abd8990ece3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled train size 61346 61346\n"
     ]
    }
   ],
   "source": [
    "train_tuple = list(set((zip(train_in_sentences,train_sent_label))))\n",
    "train_sum_sent = 3 * [stan for stan,label in train_tuple if label==1]\n",
    "train_nonsum_sent = [stan for stan,label in train_tuple if label==0]\n",
    "train_in_sentences = train_sum_sent + train_nonsum_sent\n",
    "train_sent_label = len(train_sum_sent)*[1] + len(train_nonsum_sent)*[0]\n",
    "print(\"Oversampled train size\",len(train_in_sentences),len(train_sent_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "arranged-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "label0_sent = [i for i, j in zip(train_in_sentences, train_sent_label) if j == 0]\n",
    "label1_sent = [i for i, j in zip(train_in_sentences, train_sent_label) if j == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "opponent-colorado",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46271 15075\n"
     ]
    }
   ],
   "source": [
    "print(len(label0_sent), len(label1_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "stqpLSgREGaJ",
   "metadata": {
    "id": "stqpLSgREGaJ"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "handed-blowing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "handed-blowing",
    "outputId": "5f946abf-1cc6-41fe-b421-deb161ad8cc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SciBERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading SciBERT tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "removed-sunday",
   "metadata": {
    "id": "removed-sunday"
   },
   "outputs": [],
   "source": [
    "train_pairs = [(i, j) for i, j in zip(train_in_sentences, train_sent_label)]\n",
    "validation_pairs = [(i, j) for i, j in zip(valid_in_sentences, valid_sent_label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "perceived-contributor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "perceived-contributor",
    "outputId": "9c6f438d-b3d9-4516-e6c9-f3c664fca4d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the gated recurrent unit variant of lstm is used throughout our model .', 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bored-hypothesis",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bored-hypothesis",
    "outputId": "8c811819-b05c-4ab3-cf38-06876e108155"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  401\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "less256 = 0\n",
    "less512 = 0\n",
    "less128 = 0\n",
    "token_lengths = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_in_sentences:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "\n",
    "    # Update the maximum sentence length.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "    if len(input_ids) < 512:\n",
    "        less512 += 1\n",
    "    \n",
    "    if len(input_ids) < 256:\n",
    "        less256 += 1\n",
    "    \n",
    "    if len(input_ids) < 128:\n",
    "        less128 += 1\n",
    "    \n",
    "    token_lengths.append(len(input_ids))\n",
    "\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "noble-utilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61346"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "distant-mexican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61346 61344 61287\n"
     ]
    }
   ],
   "source": [
    "print(less512, less256, less128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "stretch-difficulty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9990382421021745"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "61287 / 61346"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "built-throat",
   "metadata": {
    "id": "built-throat"
   },
   "outputs": [],
   "source": [
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "train_input_ids = []\n",
    "train_lengths = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in train_in_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        return_length=True,\n",
    "                        padding = \"max_length\",\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    train_lengths.append(encoded_dict['length'])\n",
    "    \n",
    "\n",
    "# Convert the lists into tensors.\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_sent_label)\n",
    "train_lengths = torch.tensor(train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "developmental-secret",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bored-equivalent",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bored-equivalent",
    "outputId": "532005e2-c140-4042-ad2f-1536b398dcbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for i in train_input_ids if len(i) < 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "popular-cowboy",
   "metadata": {
    "id": "popular-cowboy"
   },
   "outputs": [],
   "source": [
    "validation_input_ids = []\n",
    "validation_attention_masks = []\n",
    "validation_input_ids = []\n",
    "validation_lengths = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in valid_in_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        return_length=True,\n",
    "                        padding = \"max_length\",\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    validation_input_ids.append(encoded_dict['input_ids'])\n",
    "    validation_lengths.append(encoded_dict['length'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    validation_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "validation_input_ids = torch.cat(validation_input_ids, dim=0)\n",
    "validation_attention_masks = torch.cat(validation_attention_masks, dim=0)\n",
    "validation_labels = torch.tensor(valid_sent_label)\n",
    "validation_lengths = torch.tensor(validation_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "isolated-rescue",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isolated-rescue",
    "outputId": "3e177903-975e-4e7b-b183-ee00ebec5ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10681 10681 10681 10681\n"
     ]
    }
   ],
   "source": [
    "print(len(validation_input_ids), len(validation_attention_masks), len(validation_labels), len(validation_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "greenhouse-technical",
   "metadata": {
    "id": "greenhouse-technical"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brown-repair",
   "metadata": {
    "id": "brown-repair"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_lengths, train_labels)\n",
    "validation_dataset = TensorDataset(validation_input_ids, validation_attention_masks, validation_lengths, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conditional-calcium",
   "metadata": {
    "id": "conditional-calcium"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(validation_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mechanical-entertainment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mechanical-entertainment",
    "outputId": "5d75f42b-9216-466c-fbb7-5ccee4f87444"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
    "# linear classification layer on top. \n",
    "model = AutoModelForSequenceClassification.from_pretrained('allenai/scibert_scivocab_uncased', output_attentions=False, output_hidden_states = False, num_labels = 2)\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#     \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "#     num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "#                     # You can increase this for multi-class tasks.   \n",
    "#     output_attentions = False, # Whether the model returns attentions weights.\n",
    "#     output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "# )\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "XIJ4Tyuecqog",
   "metadata": {
    "id": "XIJ4Tyuecqog"
   },
   "outputs": [],
   "source": [
    "# Create a SciBERT + Bi-LSTM model for binary classification\n",
    "class NeuralNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased',  output_hidden_states=True)\n",
    "#         self.lstm = torch.nn.LSTM(768, 400, num_layers=2, batch_first = True, bidirectional=True)\n",
    "#         self.l1 = torch.nn.Linear(768, 800)  \n",
    "#         self.l2 = torch.nn.Dropout(0.1)\n",
    "#         self.l3 = torch.nn.Linear( 800, 200)\n",
    "#         self.l4 = torch.nn.Linear(200,2)\n",
    "        \n",
    "        self.l1 = torch.nn.Sequential(torch.nn.Dropout(0.1), torch.nn.Linear(768, 800), torch.nn.Dropout(0.1), torch.nn.ReLU(), torch.nn.Linear(800, 200), torch.nn.ReLU(), torch.nn.Linear(200, 2))\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        encoded_layers = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers[12]\n",
    "        scibert_hidden_layer = scibert_hidden_layer[:,0,:].view(-1,768)\n",
    "\n",
    "#         enc_hiddens, (last_hidden, last_cell) = self.lstm(torch.nn.utils.rnn.pack_padded_sequence(scibert_hidden_layer, lengths.cpu(), batch_first=True, enforce_sorted=False)) #enforce_sorted=False  #pack_padded_sequence(data and batch_sizes\n",
    "#         output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "#         output_hidden = self.l1(scibert_hidden_layer)\n",
    "#         output_hidden = self.l2(output_hidden)\n",
    "#         output_2 = self.l3(output_hidden)\n",
    "#         output_3 = torch.nn.ReLU()(output_2)\n",
    "#         output_4 = self.l4(output_3)\n",
    "        \n",
    "        output = self.l1(scibert_hidden_layer)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "uwtAVrw6fRMa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwtAVrw6fRMa",
    "outputId": "00f4a13e-2c18-4a08-a32c-13e65ae500b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l1): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=800, bias=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=800, out_features=200, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "qKMKhRhdncLx",
   "metadata": {
    "id": "qKMKhRhdncLx"
   },
   "outputs": [],
   "source": [
    "label0_sent = [i for i, j in zip(train_sentences, train_labels) if j == 0]\n",
    "label1_sent = [i for i, j in zip(train_sentences, train_labels) if j == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "I_Rf5x9DnsWB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_Rf5x9DnsWB",
    "outputId": "b6850830-8722-4eda-e1bd-b45f438c7b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40126 15075\n"
     ]
    }
   ],
   "source": [
    "print(len(label0_sent), len(label1_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bxa6h-5WnCjE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxa6h-5WnCjE",
    "outputId": "556b7d3e-a449-4d9e-91ce-9ba6412a32cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2731, 0.7269])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_weights = [len(label1_sent), len(label0_sent)]\n",
    "c_weights = [i/sum(c_weights) for i in c_weights]\n",
    "\n",
    "c_weights = torch.tensor(c_weights)\n",
    "c_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9Hw7U7r-nn0y",
   "metadata": {
    "id": "9Hw7U7r-nn0y"
   },
   "outputs": [],
   "source": [
    "CELoss = torch.nn.CrossEntropyLoss(weight = c_weights.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "consolidated-algebra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "consolidated-algebra",
    "outputId": "53c51423-80ba-4ac9-c155-22b401233c18"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a1a79634659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Get all of the model's parameters as a list of tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The BERT model has {:} different named parameters.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ultimate-mortgage",
   "metadata": {
    "id": "ultimate-mortgage"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "normal-green",
   "metadata": {
    "id": "normal-green"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
    "# We chose to run for 4, but we'll see later that this may be over-fitting the\n",
    "# training data.\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "hundred-attendance",
   "metadata": {
    "id": "hundred-attendance"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def results(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy =  np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "    \n",
    "    f1 = f1_score(labels, pred_flat)\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "measured-domain",
   "metadata": {
    "id": "measured-domain"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "czech-choice",
   "metadata": {
    "id": "czech-choice"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ZmbY6hnDFqxU",
   "metadata": {
    "id": "ZmbY6hnDFqxU"
   },
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#   break\n",
    "\n",
    "# ids = batch[0].to(device)\n",
    "# masks = batch[1].to(device)\n",
    "# lens = batch[2].to(device)\n",
    "# labels = batch[3].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "RaD03tZdFrAX",
   "metadata": {
    "id": "RaD03tZdFrAX"
   },
   "outputs": [],
   "source": [
    "# outputs = model(ids, masks, None, lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "OZQbn7EhFrC0",
   "metadata": {
    "id": "OZQbn7EhFrC0"
   },
   "outputs": [],
   "source": [
    "# loss = CELoss(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7jF8Jw1oGmvk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7jF8Jw1oGmvk",
    "outputId": "d2110d75-2dbb-47db-b693-dd7c66d498f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6145, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "AEhgpohgGYI0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEhgpohgGYI0",
    "outputId": "32bd775a-2d25-443e-d189-376437676981"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/NLP/Fancy-SciBERT-Oversampling/trained_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-9f9985f5e5ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/NLP/Fancy-SciBERT-Oversampling/trained_model.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/NLP/Fancy-SciBERT-Oversampling/trained_model.bin'"
     ]
    }
   ],
   "source": [
    "# model = NeuralNetwork()\n",
    "# model.load_state_dict(torch.load(\"/content/drive/MyDrive/NLP/Fancy-SciBERT-Oversampling/trained_model.bin\"))\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "understood-arlington",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "understood-arlington",
    "outputId": "d40908e8-a88c-459a-aca0-86f813421be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  1,918.    Elapsed: 0:01:26.\n",
      "  Batch   100  of  1,918.    Elapsed: 0:02:53.\n",
      "  Batch   150  of  1,918.    Elapsed: 0:04:19.\n",
      "  Batch   200  of  1,918.    Elapsed: 0:05:45.\n",
      "  Batch   250  of  1,918.    Elapsed: 0:07:12.\n",
      "  Batch   300  of  1,918.    Elapsed: 0:08:38.\n",
      "  Batch   350  of  1,918.    Elapsed: 0:10:04.\n",
      "  Batch   400  of  1,918.    Elapsed: 0:11:31.\n",
      "  Batch   450  of  1,918.    Elapsed: 0:12:58.\n",
      "  Batch   500  of  1,918.    Elapsed: 0:14:25.\n",
      "  Batch   550  of  1,918.    Elapsed: 0:15:52.\n",
      "  Batch   600  of  1,918.    Elapsed: 0:17:18.\n",
      "  Batch   650  of  1,918.    Elapsed: 0:18:44.\n",
      "  Batch   700  of  1,918.    Elapsed: 0:20:11.\n",
      "  Batch   750  of  1,918.    Elapsed: 0:21:36.\n",
      "  Batch   800  of  1,918.    Elapsed: 0:23:02.\n",
      "  Batch   850  of  1,918.    Elapsed: 0:24:29.\n",
      "  Batch   900  of  1,918.    Elapsed: 0:25:55.\n",
      "  Batch   950  of  1,918.    Elapsed: 0:27:20.\n",
      "  Batch 1,000  of  1,918.    Elapsed: 0:28:44.\n",
      "  Batch 1,050  of  1,918.    Elapsed: 0:30:09.\n",
      "  Batch 1,100  of  1,918.    Elapsed: 0:31:35.\n",
      "  Batch 1,150  of  1,918.    Elapsed: 0:32:59.\n",
      "  Batch 1,200  of  1,918.    Elapsed: 0:34:24.\n",
      "  Batch 1,250  of  1,918.    Elapsed: 0:35:49.\n",
      "  Batch 1,300  of  1,918.    Elapsed: 0:37:14.\n",
      "  Batch 1,350  of  1,918.    Elapsed: 0:38:39.\n",
      "  Batch 1,400  of  1,918.    Elapsed: 0:39:57.\n",
      "  Batch 1,450  of  1,918.    Elapsed: 0:41:07.\n",
      "  Batch 1,500  of  1,918.    Elapsed: 0:42:18.\n",
      "  Batch 1,550  of  1,918.    Elapsed: 0:43:28.\n",
      "  Batch 1,600  of  1,918.    Elapsed: 0:44:41.\n",
      "  Batch 1,650  of  1,918.    Elapsed: 0:46:06.\n",
      "  Batch 1,700  of  1,918.    Elapsed: 0:47:30.\n",
      "  Batch 1,750  of  1,918.    Elapsed: 0:48:55.\n",
      "  Batch 1,800  of  1,918.    Elapsed: 0:50:21.\n",
      "  Batch 1,850  of  1,918.    Elapsed: 0:51:46.\n",
      "  Batch 1,900  of  1,918.    Elapsed: 0:53:10.\n",
      "\n",
      "  Average training loss: 0.05\n",
      "  Training epcoh took: 0:53:40\n",
      "\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    50  of  1,918.    Elapsed: 0:01:25.\n",
      "  Batch   100  of  1,918.    Elapsed: 0:02:50.\n",
      "  Batch   150  of  1,918.    Elapsed: 0:04:14.\n",
      "  Batch   200  of  1,918.    Elapsed: 0:05:39.\n",
      "  Batch   250  of  1,918.    Elapsed: 0:07:04.\n",
      "  Batch   300  of  1,918.    Elapsed: 0:08:29.\n",
      "  Batch   350  of  1,918.    Elapsed: 0:09:54.\n",
      "  Batch   400  of  1,918.    Elapsed: 0:11:19.\n",
      "  Batch   450  of  1,918.    Elapsed: 0:12:44.\n",
      "  Batch   500  of  1,918.    Elapsed: 0:14:09.\n",
      "  Batch   550  of  1,918.    Elapsed: 0:15:34.\n",
      "  Batch   600  of  1,918.    Elapsed: 0:16:59.\n",
      "  Batch   650  of  1,918.    Elapsed: 0:18:24.\n",
      "  Batch   700  of  1,918.    Elapsed: 0:19:49.\n",
      "  Batch   750  of  1,918.    Elapsed: 0:21:14.\n",
      "  Batch   800  of  1,918.    Elapsed: 0:22:40.\n",
      "  Batch   850  of  1,918.    Elapsed: 0:24:05.\n",
      "  Batch   900  of  1,918.    Elapsed: 0:25:31.\n",
      "  Batch   950  of  1,918.    Elapsed: 0:26:56.\n",
      "  Batch 1,000  of  1,918.    Elapsed: 0:28:21.\n",
      "  Batch 1,050  of  1,918.    Elapsed: 0:29:46.\n",
      "  Batch 1,100  of  1,918.    Elapsed: 0:31:12.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-169250c9c0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, 2):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, 2))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack this training batch from our dataloader. \n",
    "        #\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "        # `to` method.\n",
    "        #\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_lengths = batch[2].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a\n",
    "        # backward pass. PyTorch doesn't do this automatically because \n",
    "        # accumulating the gradients is \"convenient while training RNNs\". \n",
    "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # In PyTorch, calling `model` will in turn call the model's `forward` \n",
    "        # function and pass down the arguments. The `forward` function is \n",
    "        # documented here: \n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n",
    "        # The results are returned in a results object, documented here:\n",
    "        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n",
    "        # Specifically, we'll get the loss (because we provided labels) and the\n",
    "        # \"logits\"--the model outputs prior to activation.\n",
    "        outputs = model(b_input_ids, \n",
    "                       b_input_mask,\n",
    "                       None)\n",
    "\n",
    "        loss = CELoss(outputs, b_labels)\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # # ========================================\n",
    "    # #               Validation\n",
    "    # # ========================================\n",
    "    # # After the completion of each training epoch, measure our performance on\n",
    "    # # our validation set.\n",
    "\n",
    "    # print(\"\")\n",
    "    # print(\"Running Validation...\")\n",
    "\n",
    "    # t0 = time.time()\n",
    "\n",
    "    # # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # # during evaluation.\n",
    "    # model.eval()\n",
    "\n",
    "    # # Tracking variables \n",
    "    # total_eval_accuracy = 0\n",
    "    # total_f1_score = 0\n",
    "    # total_eval_loss = 0\n",
    "    # nb_eval_steps = 0\n",
    "\n",
    "    # # Evaluate data for one epoch\n",
    "    # for batch in validation_dataloader:\n",
    "        \n",
    "    #     # Unpack this training batch from our dataloader. \n",
    "    #     #\n",
    "    #     # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
    "    #     # the `to` method.\n",
    "    #     #\n",
    "    #     # `batch` contains three pytorch tensors:\n",
    "    #     #   [0]: input ids \n",
    "    #     #   [1]: attention masks\n",
    "    #     #   [2]: labels \n",
    "    #     b_input_ids = batch[0].to(device)\n",
    "    #     b_input_mask = batch[1].to(device)\n",
    "    #     b_labels = batch[2].to(device)\n",
    "        \n",
    "    #     # Tell pytorch not to bother with constructing the compute graph during\n",
    "    #     # the forward pass, since this is only needed for backprop (training).\n",
    "    #     with torch.no_grad():        \n",
    "\n",
    "    #         # Forward pass, calculate logit predictions.\n",
    "    #         # token_type_ids is the same as the \"segment ids\", which \n",
    "    #         # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "    #         result = model(b_input_ids, \n",
    "    #                        token_type_ids=None, \n",
    "    #                        attention_mask=b_input_mask,\n",
    "    #                        labels=b_labels,\n",
    "    #                        return_dict=True)\n",
    "\n",
    "    #     # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
    "    #     # output values prior to applying an activation function like the \n",
    "    #     # softmax.\n",
    "    #     loss = result.loss\n",
    "    #     logits = result.logits\n",
    "            \n",
    "    #     # Accumulate the validation loss.\n",
    "    #     total_eval_loss += loss.item()\n",
    "\n",
    "    #     # Move logits and labels to CPU\n",
    "    #     logits = logits.detach().cpu().numpy()\n",
    "    #     label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    #     # Calculate the accuracy for this batch of test sentences, and\n",
    "    #     # accumulate it over all batches.\n",
    "    #     total_eval_accuracy += results(logits, label_ids)[0]\n",
    "    #     total_f1_score += results(logits, label_ids)[1]\n",
    "        \n",
    "\n",
    "    # # Report the final accuracy for this validation run.\n",
    "    # avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # avg_f1 = total_f1_score / len(validation_dataloader)\n",
    "    # print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    # print(\"  F! score: {0:.2f}\".format(avg_f1))\n",
    "\n",
    "    # # Calculate the average loss over all of the batches.\n",
    "    # avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # # Measure how long the validation run took.\n",
    "    # validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    # print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    # print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Training Time': training_time,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "uBIvszcK9VJ_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBIvszcK9VJ_",
    "outputId": "2520c2c0-72f8-4d09-a6bc-da4e8ce66ee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./NotSoFancy-SciBERT/tokenizer_config.json',\n",
       " './NotSoFancy-SciBERT/special_tokens_map.json',\n",
       " './NotSoFancy-SciBERT/vocab.txt',\n",
       " './NotSoFancy-SciBERT/added_tokens.json')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"./NotSoFancy-SciBERT/pretrained_model.bin\")\n",
    "tokenizer.save_pretrained(\"./NotSoFancy-SciBERT/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4OOitmAb9mDj",
   "metadata": {
    "id": "4OOitmAb9mDj"
   },
   "outputs": [],
   "source": [
    "for batch in validation_dataloader:\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "IFF3H7oQOTTr",
   "metadata": {
    "id": "IFF3H7oQOTTr"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "ids = batch[0].to(device)\n",
    "mask = batch[1].to(device)\n",
    "lens = batch[2].to(device)\n",
    "labels = batch[3].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  outputs = model(ids, mask, None, lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "LI2nB8Cd9Tz9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LI2nB8Cd9Tz9",
    "outputId": "f4d2e5f9-d80a-4463-e797-fae6aebbc527"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_idx.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "kwLN8YZ9ZiRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwLN8YZ9ZiRf",
    "outputId": "aa84f4d9-0ecd-4bdd-8480-fb672f3d78fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "special_tokens_map.json  tokenizer_config.json\tvocab.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /content/drive/MyDrive/Vanilla-SciBERT-Nooversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "XH78KSN5GCdn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH78KSN5GCdn",
    "outputId": "b1fecc77-0303-462e-b9e0-86abb1d4bb35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 10,688 test sentences...\n",
      "Done with 0 of 334\n",
      "Done with 10 of 334\n",
      "Done with 20 of 334\n",
      "Done with 30 of 334\n",
      "Done with 40 of 334\n",
      "Done with 50 of 334\n",
      "Done with 60 of 334\n",
      "Done with 70 of 334\n",
      "Done with 80 of 334\n",
      "Done with 90 of 334\n",
      "Done with 100 of 334\n",
      "Done with 110 of 334\n",
      "Done with 120 of 334\n",
      "Done with 130 of 334\n",
      "Done with 140 of 334\n",
      "Done with 150 of 334\n",
      "Done with 160 of 334\n",
      "Done with 170 of 334\n",
      "Done with 180 of 334\n",
      "Done with 190 of 334\n",
      "Done with 200 of 334\n",
      "Done with 210 of 334\n",
      "Done with 220 of 334\n",
      "Done with 230 of 334\n",
      "Done with 240 of 334\n",
      "Done with 250 of 334\n",
      "Done with 260 of 334\n",
      "Done with 270 of 334\n",
      "Done with 280 of 334\n",
      "Done with 290 of 334\n",
      "Done with 300 of 334\n",
      "Done with 310 of 334\n",
      "Done with 320 of 334\n",
      "Done with 330 of 334\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(validation_dataloader) * batch_size))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for idx, batch in enumerate(validation_dataloader):\n",
    "  if idx % 10 == 0:\n",
    "    print(f\"Done with {idx} of {len(validation_dataloader)}\")\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_lengths, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      outputs = model(b_input_ids, \n",
    "                       b_input_mask,\n",
    "                       None)\n",
    "      _, labels = torch.max(outputs, dim = 1)\n",
    "\n",
    "  logits = labels.cpu().numpy()\n",
    "  label_ids = b_labels.cpu().numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('    DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "MMAvCi4Fqw4n",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMAvCi4Fqw4n",
    "outputId": "52999740-e0b8-4912-c1c5-5e343737b1db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "# flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "oDTGXjXCq6ra",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDTGXjXCq6ra",
    "outputId": "426563bc-a7e8-4fcb-abaa-d55e0edc177f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      9685\n",
      "           1       0.37      0.52      0.43       996\n",
      "\n",
      "    accuracy                           0.87     10681\n",
      "   macro avg       0.66      0.71      0.68     10681\n",
      "weighted avg       0.89      0.87      0.88     10681\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "    \n",
    "print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "xQ-SDAK1s63w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xQ-SDAK1s63w",
    "outputId": "45b695f2-c5b1-4099-d73e-ff5b21c3e217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constituency_parsing\t document_classification  hypernym_discovery\r\n",
      "coreference_resolution\t entity_linking\t\t  natural_language_inference\r\n",
      "data-to-text_generation  face_alignment\t\t  README.md\r\n",
      "dependency_parsing\t face_detection\r\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_file_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "CyIY8axbrT0F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CyIY8axbrT0F",
    "outputId": "c6e2dd46-b2b4-4991-bd82-deb9b2140590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test complete constituency_parsing 9\n",
      "Test complete document_classification 21\n",
      "Test complete hypernym_discovery 9\n",
      "Test complete coreference_resolution 10\n",
      "Test complete entity_linking 17\n",
      "Test complete natural_language_inference 32\n",
      "Test complete data-to-text_generation 7\n",
      "Test complete face_alignment 19\n",
      "Test complete dependency_parsing 9\n",
      "Test complete face_detection 22\n"
     ]
    }
   ],
   "source": [
    "test_input_dir = \"datasets/test/\"\n",
    "test_list_of_folders = [\"constituency_parsing\", \"document_classification\", \"hypernym_discovery\",\n",
    "         \"coreference_resolution\", \"entity_linking\", \"natural_language_inference\", \"data-to-text_generation\", \"face_alignment\", \"dependency_parsing\", \"face_detection\"]\n",
    "test_stanza_list = []\n",
    "test_sent_num_list = []\n",
    "test_file_name_list = []\n",
    "test_stanza_len = []\n",
    "\n",
    "for fls in test_list_of_folders:\n",
    "  count=0\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        test_stanza_len.append(len(stanza_lines_list))\n",
    "        test_stanza_list.append(stanza_lines_list)\n",
    "\n",
    "      if files.endswith(\"sentences.txt\"):\n",
    "        sentence_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "        sentence_num_list = list(filter(None,sentence_file.read().splitlines())) # filter empty strings and split into lines\n",
    "        test_sent_num_list.append(sentence_num_list)\n",
    "    test_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"Test complete\",fls,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1Yc1sPDhtsPq",
   "metadata": {
    "id": "1Yc1sPDhtsPq"
   },
   "outputs": [],
   "source": [
    "test_sent_num_list = [[int(s) for s in sublist] for sublist in test_sent_num_list]\n",
    "test_sent_num_list = [list(set(x)) for x in test_sent_num_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "likely-subscriber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'constituency parsing with a self - attentive encoder',\n",
       " 'abstract',\n",
       " 'we demonstrate that replacing an lstm encoder with a self - attentive architecture can lead to improvements to a state - of the - art discriminative constituency parser .',\n",
       " 'the use of attention makes explicit the manner in which information is propagated between different locations in the sentence , which we use to both analyze our model and propose potential improvements .',\n",
       " 'for example , we find that separating positional and content information in the encoder can lead to improved parsing accuracy .',\n",
       " 'additionally , we evaluate different approaches for lexical representation .',\n",
       " 'our parser achieves new state - of the - art results for single models trained on the penn treebank : 93.55 f1 without the use of any external data , and 95.13 f1 when using pre-trained word representations .',\n",
       " 'our parser also outperforms the previous best - published accuracy figures on 8 of the 9 languages in the spmrl dataset .',\n",
       " 'introduction',\n",
       " 'in recent years , neural network approaches have led to improvements in constituency parsing .',\n",
       " 'many of these parsers can broadly be characterized as following an encoder - decoder design : an encoder reads the input sentence and summarizes it into a vector or set of vectors ( e.g. one for each word or span in the sentence ) , and then a decoder uses these vector summaries to incrementally buildup a labeled parse tree .',\n",
       " 'in contrast to the large variety of decoder architectures investigated in recent work , the encoders in recent parsers have predominantly been built using recurrent neural networks ( rnns ) , and in particular long short - term memory networks ( lstms ) .',\n",
       " 'rnns have largely replaced approaches such as the fixed - window - size feed - forward networks of in part due to their ability to capture global context .',\n",
       " 'however , rnns are not the only architecture capable of summarizing large global contexts : recent work by presented a new state - of - the - art approach to machine translation with an architecture that entirely eliminates recurrent connections and relies instead on a repeated neural attention mechanism .',\n",
       " 'in this paper , we introduce a parser that combines an encoder built using this kind of self - attentive architecture with a decoder customized for parsing ( ) .',\n",
       " 'in section 2 of this paper , we describe the architecture and present our finding that self - attention can outperform an lstm - based approach .',\n",
       " 'a neural attention mechanism makes explicit the manner in which information is transferred between different locations in the sentence , which we can use to study the relative importance of different kinds of context to the parsing task .',\n",
       " 'different locations in the sentence can attend to each other based on their positions , but also based on their contents ( i.e. based on the words at or around those positions ) .',\n",
       " 'in section 3 we present our find - ing that when our parser learns to make an implicit trade - off between these two types of attention , it predominantly makes use of position - based attention , and show that explicitly factoring the two types of attention can noticeably improve parsing accuracy .',\n",
       " \"in section 4 , we study our model 's use of attention and reaffirm the conventional wisdom that sentence - wide global context is important for parsing decisions .\",\n",
       " 'like in most neural parsers , we find morphological ( or at least sub - word ) features to be important to achieving good results , particularly on unseen words or inflections .',\n",
       " 'in section 5.1 , we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes / suffixes can outperform using part - of - speech tags from an external system .',\n",
       " 'we also present a version of our model that uses a character lstm , which performs better than other lexical representationseven if word embeddings are removed from the model .',\n",
       " 'in section 5.2 , we explore an alternative approach for lexical representations that makes use of pre-training on a large unsupervised corpus .',\n",
       " 'we find that using the deep contextualized representations proposed by can boost parsing accuracy .',\n",
       " 'our parser achieves 93.55 f1 on the penn treebank wsj test set when not using external word representations , outperforming all previous singlesystem constituency parsers trained only on the wsj training set .',\n",
       " 'the addition of pre-trained word representations following increases parsing accuracy to 95.13 f1 , a new stateof - the - art for this dataset .',\n",
       " 'our model also outperforms previous best published results on 8 of the 9 languages in the spmrl 2013 / 2014 shared tasks .',\n",
       " 'code and trained english models are publicly available .',\n",
       " '1',\n",
       " 'base model',\n",
       " 'our parser follows an encoder - decoder architecture , as shown in .',\n",
       " 'the decoder , described in section 2.1 , is borrowed from the chart parser of with additional modifications from .',\n",
       " 'their parser is architecturally streamlined yet achieves the highest performance among discriminative single - system parsers trained on wsj data only , which is why we selected it as the starting point for our experiments with encoder variations .',\n",
       " 'sections 2.2 and 2.3 de-scribe the base version of our encoder , where the self - attentive architecture described in section 2.2 is adapted from .',\n",
       " 'tree scores and chart decoder',\n",
       " 'our parser assigns a real - valued score s ( t ) to each tree t , which decomposes as',\n",
       " 'here s ( i , j , l ) is a real - valued score for a constituent that is located between fencepost positions i and j in a sentence and has label l .',\n",
       " 'to handle unary chains , the set of labels includes a collapsed entry for each unary chain in the training set .',\n",
       " 'the model handles n-ary trees by binarizing them and introducing a dummy label ?',\n",
       " 'to nodes created during binarization , with the property that ? i , j : s ( i , j , ? ) = 0 . enforcing that scores associated with the dummy labels are always zero ensures that here ?',\n",
       " 'is the hamming loss on labeled spans , and the tree corresponding to the most -violated constraint can be found using a slight modification of the inference algorithm used at test time .',\n",
       " 'for further details , see .',\n",
       " 'the remainder of this paper concerns itself with the functional form of s ( i , j , l ) , which is calculated using a neural network for all l = ?.',\n",
       " 'context - aware word representations',\n",
       " 'the encoder portion of our model is split into two parts : a word - based portion that assigns a contextaware vector representation y t to each position tin the sentence ( described in this section ) , and a chart portion that combines the vectors y t to generate span scores s ( i , j , l) ( section 2.3 ) .',\n",
       " 'the architecture for generating the vectors y t is adapted from . :',\n",
       " 'an overview of our encoder , which produces a context - aware summary vector for each word in the sentence .',\n",
       " 'the multi-headed attention mechanism is the only means by which information may propagate between different positions in the sentence .',\n",
       " 'multi - head attention',\n",
       " 'the encoder takes as input a sequence of word embeddings [ w 1 , w 2 , . . . , w t ] , where the first and last embeddings are of special start and stop tokens .',\n",
       " 'all word embeddings are learned jointly with other parts of the model .',\n",
       " 'to better generalize to words thatare not seen during training , the encoder also receives a sequence of part - of - speech tag embeddings [ m 1 , m 2 , . . . , m t ] based on the output of an external tagger ( alternative lexical representations are discussed in section 5 ) .',\n",
       " 'additionally , the encoder stores a learned table of position embeddings , where every number i ? 1 , 2 , . . . ( up to some maximum sentence length ) is associated with a vector pi .',\n",
       " 'all embeddings have the same dimensionality , which we call d model , and are added together at the input of the encoder :',\n",
       " 'the vectors [ z 1 , z 2 , . . . , z t ] are transformed by a stack of 8 identical layers , as shown in .',\n",
       " 'each layer consists of two stacked sublayers : a multi-headed attention mechanism and a positionwise feed - forward sublayer .',\n",
       " 'the output of each sublayer given an input x is layernorm ( x + sublayer ( x ) ) ,',\n",
       " 'i.e. each sublayer is followed by a residual connection and a layer normalization step .',\n",
       " 'as a result , all sublayer outputs , including final outputs y t , are of size d model .',\n",
       " 'self - attention',\n",
       " 'the first sublayer in each of our 8 layers is a multi-headed self - attention mechanism , which is the only means by which information may propagate between positions in the sentence .',\n",
       " 'the input an input x t is split into three vectors that participate in the attention mechanism : a query qt , a key kt , and a value v t .',\n",
       " 'the query qt is compared with all keys to form a probability distribution p ( t ? ) , which is then used to retrieve an average valuev t .',\n",
       " 'to the attention mechanism is a t d model matrix x , where each row vector x t corresponds to word tin the sentence .',\n",
       " 'we first consider a single attention head , as illustrated in .',\n",
       " 'learned parameter matrices w q , w k , and wv are used to map an input x t to three vectors : a query qt = w q x t , a key kt = w k x t , and a value v t = wv x t .',\n",
       " 'query and key vectors have the same number of dimensions , which we call d k .',\n",
       " 'the probability that word i attends to word j is then calculated as p ( i ? j ) ? exp (',\n",
       " '.',\n",
       " 'the values v j for all words that have been attended to are aggregated to form an average valuev i = j p ( i ? j ) v j , which is projected back to size d model using a learned matrix w o .',\n",
       " 'in matrix form , the behavior of a single attention head is :',\n",
       " 'rather than using a single head , our model sums together the outputs from multiple heads :',\n",
       " 'each of the 8 heads has its own trainable parameters w',\n",
       " 'o .',\n",
       " 'this allows a word to gather information from up to 8 remote locations in the sentence at each attention sublayer .',\n",
       " 'position - wise feed - forward sublayer',\n",
       " 'we use the same form as :',\n",
       " 'here relu denotes the rectified linear unit nonlinearity , and distinct sets of learned parameters are used at each of the 8 instances of the feedforward sublayer in our model .',\n",
       " 'the input and output dimensions are the same because of the use of residual connections throughout the model , but we can vary the number of parameters by adjusting the size of the intermediate vector that the nonlinearity is applied to .',\n",
       " 'span scores',\n",
       " 'the outputs y t from the word - based encoder portion described in the previous section are combined to form span scores s ( i , j , ) following the method of .',\n",
       " 'concretely ,',\n",
       " '] combines summary vectors for relevant positions in the sentence .',\n",
       " 'a span endpoint to the right of the word potentially requires different information from the endpoint to the left , so a word at a position k is associated with two annotation vectors ( y k by splitting in half 2 the outputs y k from section 2.2 .',\n",
       " 'we also introduce a layer normalization step to match the use of layer normalization throughout our model .',\n",
       " 'results',\n",
       " 'the model presented above achieves a score of 92.67 f1 on the penn treebank wsj development set .',\n",
       " 'details regarding hyperparameter choice and optimizer settings are presented in appendix a.',\n",
       " 'for comparison , a model that uses the same decode procedure with an lstm - based encoder achieves a development set score of 92.24 .',\n",
       " 'these results demonstrate that an rnn - based encoder is not required for building a 2 to avoid an adverse interaction with material described in section 3 , when a vector y k is split in half the even coordinates contribute to ?',\n",
       " 'y k and the odd coordinates contribute to ?',\n",
       " 'y k .',\n",
       " 'good parser ; in fact , self - attention can achieve better results .',\n",
       " 'content vs. position attention',\n",
       " 'to examine the relative utilization of contentbased vs. position - based attention in our architecture , we perturb a trained model at test - time by selectively zeroing out the contribution of either the content or the position component to any attention mechanism .',\n",
       " 'this can be done independently at different layers ; the results of this experiment are shown in .',\n",
       " 'we can see that our model learns to use a combination of the two attention types , with positionbased attention being the most important .',\n",
       " 'we also see that content - based attention is more useful at later layers in the network , which is consistent with the idea that the initial layers of our model act similarly to a dilated convolutional network while the upper layers have a greater balance between the two attention types .',\n",
       " 'attention',\n",
       " 'content',\n",
       " 'position f1',\n",
       " 'analysis of our model',\n",
       " 'the defining feature of our encoder is the use of self - attention , which is the only mechanism for transfer of information between different locations throughout a sentence .',\n",
       " 'the attention is further factored into types : content - based attention and position - based attention .',\n",
       " 'in this section , we analyze the manner in which our model uses this attention mechanism to make its predictions .',\n",
       " 'windowed attention',\n",
       " \"we can also examine our model 's use of longdistance context information by applying window - :\",\n",
       " 'development - set f1 scores when attention is constrained to not exceed a particular distance in the sentence at test time only .',\n",
       " 'in the relaxed setting , the first and last two tokens of the sentence can attend to any word and be attended to by any word , to allow for sentence - wide pooling of information .',\n",
       " 'ing to the attention mechanism .',\n",
       " 'we begin by taking our trained model and windowing the attention mechanism at test - time only .',\n",
       " 'as shown in table 2 , strict windowing yields poor results : even a window of size 40 causes a loss in parsing accuracy compared to the original model .',\n",
       " 'when we began to investigate how the model makes use of long - distance attention , we immediately found that there are particular attention heads at some layers in our model that almost always attend to the start token .',\n",
       " \"this suggests that the start token is being used as the location for some sentence - wide pooling / processing , or perhaps as a dummy target location when ahead fails to find the particular phenomenon that it 's learned to search for .\",\n",
       " 'in light of this observation , we introduce a relaxed variation on the windowing scheme , where the start token , first word , last word , and stop token can participate in all possible uses of attention , but pairs of other words in the sentence can only attend to each other if they are within a given window .',\n",
       " 'we include three other positions in addition to the start token to do our best to cover possible locations for global pooling by our model .',\n",
       " 'results for relaxed windowing at test - time only are also shown in .',\n",
       " 'even when we allow global processing to take place at designated locations such as the start token , our model is able to make use of long - distance dependencies at up to length 40 .',\n",
       " \"next , we examine whether the parser 's use of long - distance dependencies is actually essential to performing the task by retraining our model subject to windowing .\",\n",
       " 'to evaluate the role of global : development - set f1 scores when attention is constrained to not exceed a particular distance in the sentence during training and at test time .',\n",
       " 'in the relaxed setting , the first and last two tokens of the sentence can attend to any word and be attended to by any word , to allow for sentencewide pooling of information .',\n",
       " 'computation , we consider both strict and relaxed windowing .',\n",
       " 'in principle we could have replaced relaxed windowing at training time with explicit provisions for global computation , but for analysis purposes we choose to minimize departures from our original architecture .',\n",
       " 'the results , shown in , demonstrate that long - distance dependencies continue to be essential for achieving maximum parsing accuracy using our model .',\n",
       " 'note that when a window of size 10 was imposed at training time , this was per-layer and the series of 8 layers actually had an effective context size of around 80 - which was still insufficient to recover the performance of our full parser ( with either approach to windowing ) .',\n",
       " 'the sideby - side comparison of strict and relaxed windowing shows that the ability to pool global information , using the designated locations thatare always available in the relaxed scheme , consistently translates to accuracy gains but is insufficient to compensate for small window sizes .',\n",
       " 'this suggests that not only must the information signal from longdistance tokens be available in principle , but that it also helps to have this information be directly accessible without an intermediate bottleneck .',\n",
       " 'lexical models',\n",
       " 'the models described in previous sections all rely on pretagged input sentences , where the tags are predicted using the stanford tagger .',\n",
       " 'we use the same pretagged dataset as .',\n",
       " 'in this section we explore two alternative classes of lexical models : those that use no external systems or data of any kind , as well as word vectors thatare pretrained in an unsupervised manner . :',\n",
       " 'development - set f1 scores for different approaches to handling morphology , with and without the addition of learned word embeddings .',\n",
       " 'models with subword features',\n",
       " 'if tag embeddings are removed from our model and only word embeddings remain ( where word embeddings are learned jointly with other model parameters ) , performance suffers by around 1 f1 .',\n",
       " 'to restore performance without introducing any dependencies on an external system , we explore incorporating lexical features directly into our model .',\n",
       " 'the results for different approaches we describe in this section are shown in .',\n",
       " 'we first evaluate an approach ( charlstm ) that independently runs a bidirectional lstm over the characters in each word and uses the lstm outputs in place of part - of - speech tag embeddings .',\n",
       " 'we find that this approach performs better than using predicted part - of - speech tags .',\n",
       " 'we can further remove the word embeddings ( leaving the character lstms only ) , which does not seem to hurt and can actually help increase parsing accuracy .',\n",
       " 'next we examine the importance of recurrent connections by constructing and evaluating a simpler alternative .',\n",
       " 'our approach ( charconcat ) is inspired by , who found it effective to replace words with frequently - occurring suffixes , and the observation that our original tag embeddings are rather high - dimensional .',\n",
       " 'to represent a word , we extract its first 8 letters and last 8 letters , embed each letter , and concatenate the results .',\n",
       " 'if we use 32 - dimensional embeddings , the 16 letters can be packed into a 512 - dimensional vector - the same size as the inputs to our model .',\n",
       " 'this size for the inputs in our model was chosen to simplify the use of residual connections ( by matching vector dimensions ) , even though the inputs themselves could have been encoded in a smaller vector .',\n",
       " 'this allows us to directly replace tag embeddings with the 16 - letter prefix / suffix concatenation .',\n",
       " 'for short words , embeddings of a padding token are inserted as needed .',\n",
       " 'words longer than 16 letters are represented in a lossy manner by this concatenative approach , but we hypothesize that prefix / suffix information is enough for our task .',\n",
       " 'we find this simple scheme remarkably effective : it is able to outperform pretagging and can operate even in the absence of word embeddings .',\n",
       " 'however , its performance is ultimately not quite as good as using a character lstm .',\n",
       " 'given the effectiveness of the self - attentive encoder at the sentence level , it is aesthetically appealing to consider it as a sub-word architecture as well .',\n",
       " 'however , it was empirically much slower , did not parallelize better than a character - level lstm ( because words tend to be short ) , and initial results underperformed the lstm .',\n",
       " 'one explanation is that in a lexical model , one only wants to compute a single vector per word , whereas the self - attentive architecture is better adapted for producing context - aware summaries at multiple positions in a sequence .',\n",
       " 'external embeddings',\n",
       " 'next , we consider a version of our model that uses external embeddings .',\n",
       " 'recent work by has achieved state - of - the - art performance across a range of nlp tasks by augmenting existing models with a new technique for word representation called elmo ( embeddings from language models ) .',\n",
       " 'their approach is able to capture both subword information and contextual clues : the embeddings are produced by a network that takes characters as input and then uses an lstm to capture contextual information when producing a vector representation for each word in a sentence .',\n",
       " 'we evaluate a version of our model that uses elmo as the sole lexical representation , using publicly available elmo weights .',\n",
       " 'these pre-trained word representations are 1024 dimensional , whereas all of our factored models thus far have 512 - dimensional content representations ; we found that the most effective way to address this mismatch is to project the elmo vectors to the required dimensionality using a learned weight matrix .',\n",
       " 'with the addition of contextualized word representations , we hypothesized that a full 8 layers of self - attention would no longer be necessary .',\n",
       " 'this proved true in practice : our best development set result of 95.21 f1 was obtained with a 4 - layer encoder .',\n",
       " '6 results',\n",
       " 'english ( wsj )',\n",
       " 'the development set scores of the parser variations presented in previous sections are summarized in .',\n",
       " 'our best - performing parser used a factored self - attentive encoder over elmo word representations .',\n",
       " 'the results of evaluating our model on the test set are shown in .',\n",
       " 'the test score of 93.55 f1 for our charlstm parser exceeds the previous best numbers for single - system parsers trained on the penn treebank ( without the use of any external data , such as pre-trained word embeddings ) .',\n",
       " 'when our parser is augmented with elmo word representations , it achieves a new state - of - the - art score of 95.13 f1 on the wsj test set .',\n",
       " 'our wsj - only parser took 18 hours to train using a single tesla k80 gpu and can parse the',\n",
       " 'multilingual ( spmrl )',\n",
       " \"we tested our model 's ability to generalize across languages by training it on the nine languages represented in the spmrl 2013 / 2014 shared tasks .\",\n",
       " 'to verify that our lexical representations can function for morphologicallyrich languages and smaller treebanks , we restricted ourselves to running a subset of the exact models that we evaluated on english .',\n",
       " 'in particular , we evaluated the model that uses a character - level lstm , with and without the addition of learned word embeddings .',\n",
       " 'we did not evaluate elmo in the multilingual setting because pre-trained elmo weights were only available for english .',\n",
       " 'hyperparameters were unchanged compared to the english model with the exception of the learning rate , which we adjusted for some of the smaller datasets in the spmrl task ( see appendix a ) .',\n",
       " 'results are shown in .',\n",
       " 'development set results show that the addition of word embeddings to a model that uses a character lstm has a mixed effect : it improves performance for some languages , but hurts for others .',\n",
       " 'for each language , we selected the trained model that performed better on the development set and evaluated it on the test set .',\n",
       " 'on 8 of the 9 languages , our test set result exceeds the previous best - published numbers from any system we are aware of .',\n",
       " 'the exception is swedish , where the model of continues to be state - of - the - art despite a number of approaches proposed in the intervening years that have achieved better performance on other languages .',\n",
       " 'we note that their model uses ensembling ( via product grammars ) and a reranking step , whereas our model was only evaluated in the single - system condition .',\n",
       " 'conclusion',\n",
       " 'in this paper , we show that the choice of encoder can have a substantial effect on parser performance .',\n",
       " 'in particular , we demonstrate state - of - theart parsing results with a novel encoder based on factored self - attention .',\n",
       " 'the gains we see come not only from incorporating more information ( such as subword features or externally - trained word representations ) , but also from structuring the architecture to separate different kinds of information from each other .',\n",
       " 'our results suggest that further research into different ways of encoding utterances can lead to additional improvements in both parsing and other natural language processing tasks .',\n",
       " 'a training details',\n",
       " 'a.1 model hyperparameters',\n",
       " 'the hyperparameters for our model are shown in .',\n",
       " 'hyperparameters were tuned on the development set for english .',\n",
       " 'a.2 optimizer parameters',\n",
       " 'our model was trained using adam with a batch size of 250 sentences .',\n",
       " 'for the first 160 batches ( equal to 1 epoch for english ) , the learning rate was increased linearly from 0 up to the base learning rate shown in .',\n",
       " 'development - set performance was evaluated four times per epoch ; if it did not improve for 5 epochs in a row the learning rate was halved .',\n",
       " 'the iterate that performed best on the development set was taken as the output of the training procedure .',\n",
       " 'to ensure stability of the optimizer , we found it important to use a large batch size , to warm up the learning rate overtime ( similar to ) , and to pick an appropriate learning rate .',\n",
       " 'a.3 position embeddings',\n",
       " 'all variations of our model use learned position embeddings .',\n",
       " 'our attempts to use the sinusoidal position embeddings proposed by consistently performed worse than using learned embeddings .',\n",
       " 'language base learning rate',\n",
       " 'english 0.0008 hebrew 0.002 polish 0.0015 swedish 0.002',\n",
       " 'all others 0.0008 : learning rates .']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stanza_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "KiOCFuuhuAoA",
   "metadata": {
    "id": "KiOCFuuhuAoA"
   },
   "outputs": [],
   "source": [
    "multihot_test_sent = []\n",
    "\n",
    "for i in range(len(test_stanza_list)):\n",
    "  temp = [0] * test_stanza_len[i]\n",
    "  for j in range(len(test_sent_num_list[i])):\n",
    "    t1 = test_sent_num_list[i][j] - 1\n",
    "    temp[t1] = 1\n",
    "  multihot_test_sent.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "TaaWb-NjvEHw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaaWb-NjvEHw",
    "outputId": "41b7f2e1-572e-41c6-a8b3-594d87acc6fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size 31619 31619\n"
     ]
    }
   ],
   "source": [
    "#### Flatten the list and use heuristic to remove vague sentences\n",
    "#### Train set\n",
    "from collections import Counter\n",
    "\n",
    "test_sentences = list(flatten(test_stanza_list))\n",
    "test_label = list(flatten(multihot_test_sent))\n",
    "\n",
    "test_tuple = list(set((zip(test_sentences, test_label))))\n",
    "test_sentences = []\n",
    "test_sent_label = []\n",
    "for stan,lab in test_tuple:\n",
    "  if len(stan) >4:\n",
    "    test_sentences.append(stan)\n",
    "    test_sent_label.append(lab)\n",
    "\n",
    "\n",
    "print(\"test size\",len(test_sentences),len(test_sent_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "lK0YT6vovlYa",
   "metadata": {
    "id": "lK0YT6vovlYa"
   },
   "outputs": [],
   "source": [
    "test_attention_masks = []\n",
    "test_ids = []\n",
    "test_lens = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in test_sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 256,           # Pad & truncate all sentences.\n",
    "                        truncation=True,\n",
    "                        padding = \"max_length\",\n",
    "                        return_length=True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list.    \n",
    "    test_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding).\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "    test_lens.append(encoded_dict['length'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "test_ids = torch.cat(test_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_sent_label = torch.tensor(test_sent_label)\n",
    "test_lens = torch.tensor(test_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "s_B0aERGwUpf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_B0aERGwUpf",
    "outputId": "6b943956-8661-48be-fc13-855b03a4b885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31619 31619 31619 31619\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ids), len(test_attention_masks), len(test_sent_label), len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "CFF_J5YAv7B0",
   "metadata": {
    "id": "CFF_J5YAv7B0"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-9361d85d6e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_attention_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_lens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sent_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "test_dataset = TensorDataset(test_ids, test_attention_masks, test_lens, test_sent_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "qAJUyi-3wMaH",
   "metadata": {
    "id": "qAJUyi-3wMaH"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
    "# size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(test_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "LfhhTx7lwSqr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfhhTx7lwSqr",
    "outputId": "aa4a94cc-5838-4763-a5df-e121702b2102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 31,648 test sentences...\n",
      "Done with 0 of 989\n",
      "Done with 10 of 989\n",
      "Done with 20 of 989\n",
      "Done with 30 of 989\n",
      "Done with 40 of 989\n",
      "Done with 50 of 989\n",
      "Done with 60 of 989\n",
      "Done with 70 of 989\n",
      "Done with 80 of 989\n",
      "Done with 90 of 989\n",
      "Done with 100 of 989\n",
      "Done with 110 of 989\n",
      "Done with 120 of 989\n",
      "Done with 130 of 989\n",
      "Done with 140 of 989\n",
      "Done with 150 of 989\n",
      "Done with 160 of 989\n",
      "Done with 170 of 989\n",
      "Done with 180 of 989\n",
      "Done with 190 of 989\n",
      "Done with 200 of 989\n",
      "Done with 210 of 989\n",
      "Done with 220 of 989\n",
      "Done with 230 of 989\n",
      "Done with 240 of 989\n",
      "Done with 250 of 989\n",
      "Done with 260 of 989\n",
      "Done with 270 of 989\n",
      "Done with 280 of 989\n",
      "Done with 290 of 989\n",
      "Done with 300 of 989\n",
      "Done with 310 of 989\n",
      "Done with 320 of 989\n",
      "Done with 330 of 989\n",
      "Done with 340 of 989\n",
      "Done with 350 of 989\n",
      "Done with 360 of 989\n",
      "Done with 370 of 989\n",
      "Done with 380 of 989\n",
      "Done with 390 of 989\n",
      "Done with 400 of 989\n",
      "Done with 410 of 989\n",
      "Done with 420 of 989\n",
      "Done with 430 of 989\n",
      "Done with 440 of 989\n",
      "Done with 450 of 989\n",
      "Done with 460 of 989\n",
      "Done with 470 of 989\n",
      "Done with 480 of 989\n",
      "Done with 490 of 989\n",
      "Done with 500 of 989\n",
      "Done with 510 of 989\n",
      "Done with 520 of 989\n",
      "Done with 530 of 989\n",
      "Done with 540 of 989\n",
      "Done with 550 of 989\n",
      "Done with 560 of 989\n",
      "Done with 570 of 989\n",
      "Done with 580 of 989\n",
      "Done with 590 of 989\n",
      "Done with 600 of 989\n",
      "Done with 610 of 989\n",
      "Done with 620 of 989\n",
      "Done with 630 of 989\n",
      "Done with 640 of 989\n",
      "Done with 650 of 989\n",
      "Done with 660 of 989\n",
      "Done with 670 of 989\n",
      "Done with 680 of 989\n",
      "Done with 690 of 989\n",
      "Done with 700 of 989\n",
      "Done with 710 of 989\n",
      "Done with 720 of 989\n",
      "Done with 730 of 989\n",
      "Done with 740 of 989\n",
      "Done with 750 of 989\n",
      "Done with 760 of 989\n",
      "Done with 770 of 989\n",
      "Done with 780 of 989\n",
      "Done with 790 of 989\n",
      "Done with 800 of 989\n",
      "Done with 810 of 989\n",
      "Done with 820 of 989\n",
      "Done with 830 of 989\n",
      "Done with 840 of 989\n",
      "Done with 850 of 989\n",
      "Done with 860 of 989\n",
      "Done with 870 of 989\n",
      "Done with 880 of 989\n",
      "Done with 890 of 989\n",
      "Done with 900 of 989\n",
      "Done with 910 of 989\n",
      "Done with 920 of 989\n",
      "Done with 930 of 989\n",
      "Done with 940 of 989\n",
      "Done with 950 of 989\n",
      "Done with 960 of 989\n",
      "Done with 970 of 989\n",
      "Done with 980 of 989\n",
      "DONE.\n"
     ]
    }
   ],
   "source": [
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataloader) * batch_size))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for idx, batch in enumerate(test_dataloader):\n",
    "  if idx % 10 == 0:\n",
    "    print(f\"Done with {idx} of {len(test_dataloader)}\")\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_lengths, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      outputs = model(b_input_ids, \n",
    "                       b_input_mask,\n",
    "                       None)\n",
    "      _, labels = torch.max(outputs, dim = 1)\n",
    "\n",
    "  logits = labels.cpu().numpy()\n",
    "  label_ids = b_labels.cpu().numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.append(logits)\n",
    "  true_labels.append(label_ids)\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "TcOdeEr-wx86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TcOdeEr-wx86",
    "outputId": "ee10f08a-373f-4780-ad10-94e6ec87cf5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.371\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "# Combine the results across all batches. \n",
    "flat_predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# For each sample, pick the label (0 or 1) with the higher score.\n",
    "# flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "\n",
    "# Combine the correct labels for each batch into a single list.\n",
    "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Calculate the MCC\n",
    "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
    "\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "wqYXjxQPy72p",
   "metadata": {
    "id": "wqYXjxQPy72p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     28957\n",
      "           1       0.36      0.52      0.43      2662\n",
      "\n",
      "    accuracy                           0.88     31619\n",
      "   macro avg       0.66      0.72      0.68     31619\n",
      "weighted avg       0.90      0.88      0.89     31619\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(flat_true_labels, flat_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "latest-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "geological-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(31090, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l1): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=768, out_features=800, bias=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=800, out_features=200, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=200, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "verified-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of file 248 coreference_resolution/8\n",
      "size of file 127 coreference_resolution/6\n",
      "size of file 129 coreference_resolution/7\n",
      "size of file 134 coreference_resolution/3\n",
      "size of file 256 coreference_resolution/0\n",
      "size of file 235 coreference_resolution/5\n",
      "size of file 118 coreference_resolution/9\n",
      "size of file 150 coreference_resolution/2\n",
      "size of file 240 coreference_resolution/4\n",
      "size of file 147 coreference_resolution/1\n",
      "Test complete coreference_resolution 317\n",
      "size of file 220 natural_language_inference/22\n",
      "size of file 201 natural_language_inference/18\n",
      "size of file 341 natural_language_inference/31\n",
      "size of file 120 natural_language_inference/8\n",
      "size of file 244 natural_language_inference/28\n",
      "size of file 404 natural_language_inference/17\n",
      "size of file 257 natural_language_inference/25\n",
      "size of file 152 natural_language_inference/27\n",
      "size of file 138 natural_language_inference/6\n",
      "size of file 221 natural_language_inference/21\n",
      "size of file 205 natural_language_inference/12\n",
      "size of file 285 natural_language_inference/26\n",
      "size of file 230 natural_language_inference/7\n",
      "size of file 306 natural_language_inference/20\n",
      "size of file 108 natural_language_inference/10\n",
      "size of file 232 natural_language_inference/15\n",
      "size of file 201 natural_language_inference/14\n",
      "size of file 274 natural_language_inference/13\n",
      "size of file 216 natural_language_inference/3\n",
      "size of file 175 natural_language_inference/0\n",
      "size of file 248 natural_language_inference/11\n",
      "size of file 213 natural_language_inference/23\n",
      "size of file 254 natural_language_inference/5\n",
      "size of file 65 natural_language_inference/9\n",
      "size of file 137 natural_language_inference/2\n",
      "size of file 224 natural_language_inference/24\n",
      "size of file 160 natural_language_inference/19\n",
      "size of file 195 natural_language_inference/4\n",
      "size of file 192 natural_language_inference/30\n",
      "size of file 240 natural_language_inference/16\n",
      "size of file 148 natural_language_inference/1\n",
      "size of file 153 natural_language_inference/29\n",
      "Test complete natural_language_inference 349\n",
      "size of file 66 hypernym_discovery/8\n",
      "size of file 128 hypernym_discovery/6\n",
      "size of file 125 hypernym_discovery/7\n",
      "size of file 169 hypernym_discovery/3\n",
      "size of file 80 hypernym_discovery/0\n",
      "size of file 188 hypernym_discovery/5\n",
      "size of file 246 hypernym_discovery/2\n",
      "size of file 116 hypernym_discovery/4\n",
      "size of file 108 hypernym_discovery/1\n",
      "Test complete hypernym_discovery 358\n",
      "size of file 319 face_detection/18\n",
      "size of file 156 face_detection/8\n",
      "size of file 240 face_detection/17\n",
      "size of file 248 face_detection/6\n",
      "size of file 295 face_detection/21\n",
      "size of file 138 face_detection/12\n",
      "size of file 278 face_detection/7\n",
      "size of file 272 face_detection/20\n",
      "size of file 194 face_detection/10\n",
      "size of file 236 face_detection/15\n",
      "size of file 269 face_detection/14\n",
      "size of file 240 face_detection/13\n",
      "size of file 409 face_detection/3\n",
      "size of file 148 face_detection/0\n",
      "size of file 281 face_detection/11\n",
      "size of file 152 face_detection/5\n",
      "size of file 277 face_detection/9\n",
      "size of file 237 face_detection/2\n",
      "size of file 110 face_detection/19\n",
      "size of file 339 face_detection/4\n",
      "size of file 466 face_detection/16\n",
      "size of file 231 face_detection/1\n",
      "Test complete face_detection 380\n",
      "size of file 203 constituency_parsing/8\n",
      "size of file 90 constituency_parsing/6\n",
      "size of file 230 constituency_parsing/7\n",
      "size of file 210 constituency_parsing/3\n",
      "size of file 223 constituency_parsing/0\n",
      "size of file 213 constituency_parsing/5\n",
      "size of file 135 constituency_parsing/2\n",
      "size of file 144 constituency_parsing/4\n",
      "size of file 204 constituency_parsing/1\n",
      "Test complete constituency_parsing 389\n",
      "size of file 202 entity_linking/8\n",
      "size of file 269 entity_linking/6\n",
      "size of file 266 entity_linking/12\n",
      "size of file 224 entity_linking/7\n",
      "size of file 214 entity_linking/10\n",
      "size of file 259 entity_linking/15\n",
      "size of file 195 entity_linking/14\n",
      "size of file 116 entity_linking/13\n",
      "size of file 220 entity_linking/3\n",
      "size of file 257 entity_linking/0\n",
      "size of file 193 entity_linking/11\n",
      "size of file 196 entity_linking/5\n",
      "size of file 271 entity_linking/9\n",
      "size of file 203 entity_linking/2\n",
      "size of file 229 entity_linking/4\n",
      "size of file 157 entity_linking/16\n",
      "size of file 127 entity_linking/1\n",
      "Test complete entity_linking 406\n",
      "size of file 226 document_classification/18\n",
      "size of file 258 document_classification/8\n",
      "size of file 231 document_classification/17\n",
      "size of file 252 document_classification/6\n",
      "size of file 222 document_classification/12\n",
      "size of file 139 document_classification/7\n",
      "size of file 214 document_classification/20\n",
      "size of file 164 document_classification/10\n",
      "size of file 203 document_classification/15\n",
      "size of file 256 document_classification/14\n",
      "size of file 226 document_classification/13\n",
      "size of file 191 document_classification/3\n",
      "size of file 93 document_classification/0\n",
      "size of file 210 document_classification/11\n",
      "size of file 264 document_classification/5\n",
      "size of file 243 document_classification/9\n",
      "size of file 128 document_classification/2\n",
      "size of file 242 document_classification/19\n",
      "size of file 265 document_classification/4\n",
      "size of file 218 document_classification/16\n",
      "size of file 284 document_classification/1\n",
      "Test complete document_classification 427\n",
      "size of file 237 dependency_parsing/8\n",
      "size of file 114 dependency_parsing/6\n",
      "size of file 103 dependency_parsing/7\n",
      "size of file 260 dependency_parsing/3\n",
      "size of file 170 dependency_parsing/0\n",
      "size of file 285 dependency_parsing/5\n",
      "size of file 216 dependency_parsing/2\n",
      "size of file 235 dependency_parsing/4\n",
      "size of file 313 dependency_parsing/1\n",
      "Test complete dependency_parsing 436\n",
      "size of file 179 data-to-text_generation/6\n",
      "size of file 126 data-to-text_generation/3\n",
      "size of file 274 data-to-text_generation/0\n",
      "size of file 319 data-to-text_generation/5\n",
      "size of file 191 data-to-text_generation/2\n",
      "size of file 301 data-to-text_generation/4\n",
      "size of file 226 data-to-text_generation/1\n",
      "Test complete data-to-text_generation 443\n",
      "size of file 243 face_alignment/18\n",
      "size of file 284 face_alignment/8\n",
      "size of file 279 face_alignment/17\n",
      "size of file 351 face_alignment/6\n",
      "size of file 271 face_alignment/12\n",
      "size of file 214 face_alignment/7\n",
      "size of file 292 face_alignment/10\n",
      "size of file 286 face_alignment/15\n",
      "size of file 265 face_alignment/14\n",
      "size of file 297 face_alignment/13\n",
      "size of file 338 face_alignment/3\n",
      "size of file 237 face_alignment/0\n",
      "size of file 266 face_alignment/11\n",
      "size of file 285 face_alignment/5\n",
      "size of file 295 face_alignment/9\n",
      "size of file 206 face_alignment/2\n",
      "size of file 211 face_alignment/4\n",
      "size of file 329 face_alignment/16\n",
      "size of file 219 face_alignment/1\n",
      "Test complete face_alignment 462\n"
     ]
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "test_input_dir = \"datasets/test/\"\n",
    "output_dir = \"datasets/output_subtask_A/\"\n",
    "\n",
    "test_list_of_folders = [f.path.split(\"/\")[-1] for f in os.scandir(test_input_dir) if f.is_dir()]\n",
    "\n",
    "test_input_stanza_list = []\n",
    "test_file_name_list = []\n",
    "test_input_stanza_len = []\n",
    "\n",
    "MAX_LEN=100\n",
    "\n",
    "for fls in test_list_of_folders:\n",
    "  for i in os.listdir(test_input_dir + fls + '/'):\n",
    "    count=count+1\n",
    "    output_info_folder = os.path.join(output_dir,fls,str(i))\n",
    "    os.makedirs(output_info_folder)\n",
    "    os.makedirs(os.path.join(output_info_folder, \"triples\"))\n",
    "    fpath = os.path.join(output_info_folder, \"entities.txt\")\n",
    "    f = open(fpath, \"w\")\n",
    "    f.close()\n",
    "    for files in os.listdir(test_input_dir + fls + '/' + str(i)):\n",
    "#       if files.endswith(\"Grobid-out.txt\"):\n",
    "#         copyfile(test_input_dir + fls + '/' + str(i) + '/' + files, output_dir + fls + '/' + str(i) + '/' + files)\n",
    "      \n",
    "      if files.endswith(\"Stanza-out.txt\"):\n",
    "        stanza_file = open(test_input_dir + fls + '/' + str(i) + '/' + files, \"r\")\n",
    "#         copyfile(test_input_dir + fls + '/' + str(i) + '/' + files, output_dir + fls + '/' + str(i) + '/' + files)\n",
    "        stanza_lines = stanza_file.read()\n",
    "        stanza_lines_list = list(filter(None, map(lambda x:x.lower(),stanza_lines.splitlines()))) # filter empty strings and split into lines\n",
    "        test_input_stanza_len.append(len(stanza_lines_list))\n",
    "        test_input_stanza_list.append(stanza_lines_list)\n",
    "    \n",
    "    print(\"size of file\",len(stanza_lines_list),fls+'/'+str(i))\n",
    "    with torch.no_grad():\n",
    "      output_file=open(os.path.join(output_info_folder, \"sentences.txt\"),\"a\")\n",
    "      if stanza_lines_list[1] != 'abstract':\n",
    "        output_file.write(str(2) +\"\\n\")\n",
    "      for t8 in range(2,len(stanza_lines_list)):\n",
    "        if (stanza_lines_list[t8]=='conclusion') or (stanza_lines_list[t8]=='conclusions'):\n",
    "          break\n",
    "        test_input = tokenizer.encode_plus(stanza_lines_list[t8],\n",
    "                                           add_special_tokens=True,\n",
    "                                           max_length=256,\n",
    "                                           truncation=True,\n",
    "                                           return_length=True,\n",
    "                                           padding=\"max_length\",\n",
    "                                           return_attention_mask=True,   # Construct attn. masks.\n",
    "                                           return_tensors='pt',     # Return pytorch tensors.\n",
    "                                            )\n",
    "        test_ids = test_input['input_ids'].to(device, dtype = torch.long)\n",
    "        test_mask = test_input['attention_mask'].to(device, dtype = torch.long)\n",
    "        test_token_type_ids = test_input['token_type_ids'].to(device, dtype = torch.long)\n",
    "        lengths = test_input['length']\n",
    "        outputs = model(test_ids, test_mask, test_token_type_ids)\n",
    "        _, output_idx = torch.max(outputs, dim =1)\n",
    "        t_output_class = output_idx.cpu().detach().numpy().tolist()\n",
    "        output_file=open(os.path.join(output_info_folder, \"sentences.txt\"),\"a\")\n",
    "        #writing the sentences.txt file showing contribution sentences\n",
    "        if t_output_class[0] == 1:\n",
    "          output_file.write(str(t8+1) +\"\\n\")\n",
    "    test_file_name_list.append(fls + '/' + str(i))\n",
    "  print(\"Test complete\",fls,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-aging",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-dodge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-borough",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NLP-Shared-Tasks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
